# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZogIH7aFyEHUOaQKkc1QgPnG1_m-Nm0g
"""

## pip install facenet-pytorch

import torch
import torchvision
from torchvision import transforms
from facenet_pytorch import MTCNN, InceptionResnetV1
from PIL import Image
import numpy as np

# Initialize MTCNN for face detection
mtcnn = MTCNN(keep_all=True, device='cuda' if torch.cuda.is_available() else 'cpu')

# Initialize InceptionResnetV1 for feature extraction and set to evaluation mode
model = InceptionResnetV1(pretrained='vggface2').eval()

def extract_faces(image_path, mtcnn):
    """Extract faces from an image using MTCNN."""
    # Load image
    img = Image.open(r"C:\Users\arinj\Downloads\pexels-hardeep-24538729.jpg")

    # Detect faces
    boxes, _ = mtcnn.detect(img)

    if boxes is None:
        print("No faces detected.")
        return []

    # Crop faces
    faces = [img.crop((box[0], box[1], box[2], box[3])) for box in boxes]

    return faces

def get_face_embedding(face, model):
    """Get the embedding of a single face image."""
    # Convert face to RGB and resize to 160x160 for model input
    face = face.convert("RGB").resize((160, 160))

    # Preprocess face for InceptionResnetV1
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    face_tensor = transform(face).unsqueeze(0)  # Add batch dimension

    # Pass through model to get embedding
    with torch.no_grad():
        embedding = model(face_tensor)

    return embedding

def extract_faces(image_path, mtcnn):
    """Extract faces from an image using MTCNN."""
# Load image using the image_path passed to the function
    img = Image.open(r"C:\Users\arinj\Downloads\pexels-hardeep-24538729.jpg")

    # Detect faces
    boxes, _ = mtcnn.detect(img)

    if boxes is None:
        print("No faces detected.")
        return []

    # Crop faces
    faces = [img.crop((box[0], box[1], box[2], box[3])) for box in boxes]

    return faces

def get_face_embedding(face, model):
    """Get the embedding of a single face image."""
    # Convert face to RGB and resize to 160x160 for model input
    face = face.convert("RGB").resize((160, 160))

    # Preprocess face for InceptionResnetV1
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    face_tensor = transform(face).unsqueeze(0)  # Add batch dimension

    # Pass through model to get embedding
    with torch.no_grad():
        embedding = model(face_tensor)

    return embedding

def cosine_similarity(embedding1, embedding2):
    """Compute cosine similarity between two embeddings."""
    return torch.nn.functional.cosine_similarity(embedding1, embedding2).item()

# Set a similarity threshold for matching
threshold = 0.6

# Find the closest match
best_match = None
best_similarity = -1  # Initialize with a low similarity

for name, known_embedding in known_embeddings:
    similarity = cosine_similarity(unknown_embedding, known_embedding)
    print(f"Similarity with {name}: {similarity:.4f}")

    if similarity > best_similarity:
        best_similarity = similarity
        best_match = name

if best_similarity > threshold:
    print(f"Best match: {best_match} with similarity: {best_similarity:.4f}")
else:
    print("No match found.")
